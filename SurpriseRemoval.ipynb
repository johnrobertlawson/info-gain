{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise Removal\n",
    "## John R. Lawson, December 2020\n",
    "#### (Supporting material for MDPI Atmosphere paper)\n",
    "##### Coauthors: Corey K. Potvin and Kenric Nelson. Code by JRL. \n",
    "##### Work supported by CIMMS/NSSL, Norman, Oklahoma, USA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "* Write class (L63) to \n",
    "    1. Generate Lorenz time series\n",
    "    2. Discretise it in time, space, thresh, etc\n",
    "    3. Test whether IG is sensitive to:\n",
    "        a. intermittency (rho parameter)\n",
    "        b. ICs (error)\n",
    "        c. drift (model error)\n",
    "        d. window length  \n",
    "        e. percentile\n",
    "    4. Plot suite of visualisations for data    \n",
    "* Write class (InfoGain) to\n",
    "    1. Verify data with cross-entropy\n",
    "        a. Decompose into REL, DSC, UNC\n",
    "        b. Other decomps in Wilks \n",
    "    2. Verify data with BS\n",
    "    3. (Stretch goal: extend to continuous & ranked)\n",
    "    4. Plot suite of visualisations for verification\n",
    "* Write class (utils) to\n",
    "    1. Run stuff for this paper\n",
    "    2. ?\n",
    "    \n",
    "Links with Wilks textbook:\n",
    "\n",
    "* IGN v BS, or CRPS v CRIGN, for different regimes etc\n",
    "* The SS isn't proper. Is this a big deal?\n",
    "* Visualisation suite\n",
    "\n",
    "Ultimately:\n",
    "* Show IG is better for rare events\n",
    "* Kernel dressing?\n",
    "* Interpretation of ignorance removal\n",
    "* Probably show that BS and IGN give different \"winners\" when the distribution of forecasts becomes (multimodal?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "There are two issues with EFSs:\n",
    "\n",
    "1. They are often underdispersive, and do not capture low-probability events. This is remedied by increasing quantisation in probability space, $\\Delta p$, by increasing ensemble membership $N_e$.\n",
    "2. At the thunderstorm scale, grid resolution of $\\Delta x = \\textrm{3 km}$ is insuffient to resolve critical processes for e.g., tornadogenesis. This is addressed by decreasing $\\Delta x$.\n",
    "\n",
    "We have an increase in computer resources $\\delta \\varsigma > 0$ to spend. Increasing ensemble membership costs $\\varsigma (\\delta \\Delta p)$; increasing horizonal resolution is $\\frac{\\Delta x_{0}}{\\Delta x_{1}}^{3}$, where the cubed power comprises the required increase in calculations in the x- and y-dimensions, plus the required time-step decrease in the CFL criterio, as $\\Delta t \\sim 5\\Delta x$. This ignores increases in bottleneck latency and vertical resolution. (And without kernel dressing, a finer resolution demands a larger $N_e$ due to the curse of dimensionality.)\n",
    "\n",
    "Further, our evaluation schemes that are based on Mean Square Error (e.g., Fractions Skill Score, Briar Score) are approximations that insufficiently reward non-zero probabilities of rare events. It is time to boil down our problem to __information transfer__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising $\\Delta x$ versus $\\Delta p$\n",
    "------------------------------------------------\n",
    "\n",
    "Consider two end-users: an ice-cream vendor (Igor) and a crane operator (Cath). Both Igor and Cath are worried about the risk of a severe thunderstorm $\\xi$. Both Igor and Cath check the forecast, where their expected probability of the hazard is $\\mathrm{E}(\\xi)$ is 0.05. Igor and Cath incur the following costs $C$ (anticipated outgoings) and losses $L$ (surprising outgoings):\n",
    "\n",
    "|  $\\dotsb$     | Operation    | Mitigation |\n",
    "| ------------- | ------------ | -----------|\n",
    "| Hazard occurs | $L$          | $C$        |\n",
    "| Hazard does not occur | $\\dotsb$ | $C$    |\n",
    "\n",
    "Avoiding action and losing profit are the components of $C$, whereas $L$ is the unexpected outlay due to damage, liability, etc. **At what probability should Igor and Cath take avoiding action?** We also seek:\n",
    "\n",
    "* What risk threshold $p_\\textrm{crit}$ should Igor and Cath implement their mitigation plans?\n",
    "* How does this change with the rarity of the event?\n",
    "* How does this change with the ratio of cost and loss $\\frac{C}{L}$ between Igor and Cath?\n",
    "* Should we spend computer resources of more ensemble members (a finer resolution $\\Delta p$ in probability space) or a higher resolution forecast (lower $\\Delta x$)? We attempt to bound the minimum information gain required to optimise the end-user's profit. \n",
    "* Can the same optimisation help both customers in all regimes?\n",
    "\n",
    "The following computing costs apply:\n",
    " \n",
    "$$\n",
    "\\varsigma(M_1) = \\varsigma(e_0) \\big( \\frac{\\Delta x_1}{\\Delta x_0} \\big )^3 \\delta N_e + B\n",
    "$$\n",
    "\n",
    "Above, (X) says that one computing unit is defined as the cost of one original-model ($\\mathbb{M_0}$) ensemble member (at the coarse resolution); (X) states that a reduction in $\\delta x$ costs (in km). Could state $N_e$ in terms of dp instead. \n",
    "\n",
    "We need a measure of forecast skill in terms of remaining uncertainty (or ingorance) after the forecast is obtained. We will use cross-entropy (rather than Briar score; see later), where zero is a perfect score:\n",
    "\n",
    "WHAT IS N, BE SPECIFIC\n",
    "\n",
    "$$\n",
    "H_\\times(o,f) = -\\frac{1}{N} \\sum^{N_t}_{t=1} \\sum^{n}_{i=1} o_{t,i} \\log_2 f_{t,i}\n",
    "$$\n",
    "\n",
    "which can be decomposed as \n",
    "\n",
    "$$\n",
    "H_\\times(o,f) = \\underbrace{ \\frac{1}{N_t} \\sum^{N_k}_{k=1} n_k D_\\textrm{KL} (\\bar{o}_k \\| f_k) }_\\textrm{Reliability (REL)} - \n",
    "\\underbrace{ \\frac{1}{N_t} \\sum^{N_k}_{k=1} n_k D_\\textrm{KL} (\\bar{o}_k \\| \\bar{o}) }_\\textrm{Discrimination (DSC)} + \n",
    "\\underbrace{ \\frac{1}{N_t} \\sum^{N_t}_{t=1} D_\\textrm{KL} (o_t \\| \\bar{o}) }_\\textrm{Uncertainty (UNC)} + \n",
    "\\underbrace{ \\frac{1}{N_t} \\sum^{N_t}_{t=1} H(o_t) }_\\textrm{Obs. Uncertainty} \\\\\n",
    "\\forall t \\in \\{ 0,1,...,N_t \\} \\cap k \\in \\{ \\underline{\\rho},\\frac{1}{N_e},\\frac{2}{N_e},...,\\frac{N_e-1}{N_e},\\bar{\\rho} \\}\n",
    "$$\n",
    "\n",
    "where $D_{\\textrm{KL}}$ is the Kullback-Liebler divergence, denoting the uncertainty or surprise (entropy) remaining after a forecast is issued, upon verification:\n",
    "\n",
    "$$\n",
    "D_{KL}(x\\|y) = \\sum^{n}_{i=1} x_i \\log_2 \\frac{x_i}{y_i}\n",
    "$$\n",
    "\n",
    "and average surprise is represented by Shannon entropy:\n",
    "\n",
    "$$\n",
    "H(x) = -\\sum^{n}_{i=1} x_i \\log_2 x_i\n",
    "$$\n",
    "\n",
    "Probabilities are bounded to $[ \\underline{\\rho},\\bar{\\rho} ]$ to avoid divergence to infinity, but also to avoid full confidence in the face of underdispersion (overfitting). Above, $f$ and $o$ are vectors.\n",
    "\n",
    "\n",
    "A skill score is formed as\n",
    "\n",
    "$$\n",
    "\\textrm{SS} = \\frac{\\textrm{RELDSC}}{\\textrm{UNC}} +- \\frac{\\textrm{RELDSC}}{\\textrm{UNC}}\n",
    "$$\n",
    "\n",
    "(Might we want a measure of how severe the largest surprise is? How long the tail is?) The normalised components are useful for comparing the useful and detrimental information from the NWP output.\n",
    "\n",
    "With grid spacing, there is the curse of dimensionality. That doesn't exist for probability as long as spread is good enough, if kernel dressing is used. If user's threshold is 20%, they are less sensitive to reliability error than a user tolerating only 10% risk. (Or is it only for rarity of event?) For rarity of event, less likely to be captured by the ensemble member.\n",
    "\n",
    "(EFS members merely attempt to resolve basins of attraction. Want members to be around the cusp (discrim?) but also sample the outer regions well enough to pick up low-amplitude signals, rare events (reliability?).\n",
    "\n",
    "Entropy is like risk of trying to remove the big-reward (low frequency) events (x-axis is surprise, y is prob density). one continuous, smooth cusp between two basins of attraction (like Lawson 2019?) is low-surprise events divided by a highly entropic  entropy (0.5 and 0.5)\n",
    "\n",
    "Compute value as tolerance in units of missed-hit frequency,  where perhaps if DISCRIM is indeed a problem (overconfident)\n",
    "\n",
    "Compare to BS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import matplotlib as M\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "%pdb off\n",
    "\n",
    "#import matplotlib.font_manager\n",
    "#print(matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf'))\n",
    "M.rcParams['font.family'] = 'sans-serif'\n",
    "M.rcParams['font.sans-serif'] = ['Bitstream Vera Sans']\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "np.random.seed(383838)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoGain:\n",
    "    \"\"\" For computing and visualising skill scores for a probabilistic forecast.\n",
    "    Currently for two categories (binary) only. \n",
    "    \"\"\"\n",
    "    def __init__(self,f,o,fk=None):\n",
    "        \"\"\" Initialise the suite.\n",
    "        \n",
    "        Args:\n",
    "        f   : 1-D prob forecasts [0,1]*\n",
    "        o   : 1-D obs forecast [0,1], may or may not include obs uncertainty\n",
    "        fk  : 1-D array of probability levels. If None, compute automatically\n",
    "        \n",
    "        * = should be bounded to avoid divergence to infinity\n",
    "        \n",
    "        TODO:\n",
    "        * logbase needs un-hardcoding\n",
    "        * Parallelise?\n",
    "        \"\"\"\n",
    "        self.forecasts = f\n",
    "        self.observations = o\n",
    "        self.logbase = 2 \n",
    "        \n",
    "        if fk == None:\n",
    "            # Work out the fk from data\n",
    "            pass \n",
    "        else:\n",
    "            self.fk = fk \n",
    "        \n",
    "        self.COMPUTE = {'DSC':self.compute_DSC,\n",
    "                       'UNC':self.compute_UNC,\n",
    "                       'DKL':self.compute_DKL,\n",
    "                       'REL':self.compute_REL,\n",
    "                       'SS':self.compute_SS,\n",
    "                       'S':self.compute_entropy,\n",
    "                       'DSCSS':self.compute_DSCSS,\n",
    "                       'RELSS':self.compute_RELSS,}\n",
    "        \n",
    "        return\n",
    "\n",
    "    def compute(self,diag,**kwargs):\n",
    "        \"\"\" Compute a diagnostic.\n",
    "        \n",
    "        Args:\n",
    "        diag    : (str) - compute one of\n",
    "                  {'DSC','UNC','REL',\n",
    "                    'SS','DKL','S'}\n",
    "        \"\"\"\n",
    "        if diag in (\"DSC\",\"DKL\",\"REL\",):\n",
    "            return self.COMPUTE[diag](self.observations,self.forecasts)\n",
    "        elif diag in (\"SS\",):\n",
    "            return self.COMPUTE[diag]()\n",
    "        elif diag in (\"S\",):\n",
    "            return self.COMPUTE[diag](self.observations)\n",
    "        else:\n",
    "            print(\"Pick one of\",self.COMPUTE.keys())\n",
    "            \n",
    "    @staticmethod\n",
    "    def compute_DKL(x,y):\n",
    "        \"\"\" Kullback-Liebler Divergence\n",
    "\n",
    "        Args:\n",
    "        x   : 1-D (e.g., observations)\n",
    "        y   : 1-D (e.g., forecasts)\n",
    "\n",
    "        \"\"\"\n",
    "        dkl = ((1-x) * np.ma.log2((1-x)/(1-y))) + (x * np.ma.log2(x/y))\n",
    "        DKL = np.mean(dkl)\n",
    "        return DKL\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_entropy(x):\n",
    "        \"\"\" Average surprise. A series of surprise\n",
    "        values are returned if return_all is True.\n",
    "        \n",
    "        Is this right? Surprise v UNC?\n",
    "        \n",
    "        \"\"\"\n",
    "        H_all = x * np.log2(x)\n",
    "        H = N.sum(H_all)\n",
    "        return H, H_all\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_XES(x,y):\n",
    "        \"\"\" Cross-entropy score.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        XES = self.compute(\"DKL\") + self.compute(\"Ho\")\n",
    "        return XES \n",
    "\n",
    "    @staticmethod\n",
    "    def compute_DSC(x,y,fk):\n",
    "        \"\"\" Compute discrimination component of DKL.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        # p_o  : frequency of observation for this k\n",
    "        dsc = np.zeros_like(fk)\n",
    "        for nk,k in enumerate(fk):\n",
    "            ok = o[f==k]\n",
    "            ok_bar = np.mean(ok)\n",
    "            o_bar = np.mean(o)\n",
    "            dsc[nk] = ok.size * compute_DKL(ok_bar,o_bar)\n",
    "        DSC = np.sum(dsc)/o.size\n",
    "        return DSC\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_REL(x,y,fk):\n",
    "        \"\"\" Compute reliability.\n",
    "        \"\"\"\n",
    "        return REL\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_UNC(x):\n",
    "        \"\"\" Compute uncertainty component of the forecast.\n",
    "        \"\"\"\n",
    "        UNC = 0 \n",
    "        return UNC \n",
    "    \n",
    "    @staticmethod\n",
    "    def bound(x,thresh=0.01):\n",
    "        x[x<thresh] = thresh\n",
    "        x[x>(1-thresh)] = 1-thresh\n",
    "        return x \n",
    "    \n",
    "    def count_bin_size(x,fk):\n",
    "        \"\"\"From Wilks: need to bin fcst probs, setting the middle of\n",
    "        the bin as the value...\n",
    "\n",
    "        \"\"\"\n",
    "        bin_counts,bin_edges = np.histogram(x,bins=fk)\n",
    "        return bin_counts\n",
    "\n",
    "    def quantise(x,fk):\n",
    "        # I think the index is not right, need to look at bin edges?\n",
    "        quant_probs = np.digitize(x,fk)\n",
    "        # pdb.set_trace()\n",
    "        f = [fk[q-1] for q in quant_probs]\n",
    "        return f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrierScore:\n",
    "    def __init__(self,f,o,fk=None):\n",
    "        \"\"\" Initialise verification.\n",
    "        \n",
    "        Args:\n",
    "        f   : 1-D prob forecasts [0,1]*\n",
    "        o   : 1-D obs forecast [0,1], may or may not include obs uncertainty\n",
    "        fk  : 1-D array of probability levels. If None, compute automatically\n",
    "        \n",
    "        * = should be bounded to avoid divergence to match IG verification\n",
    "        \n",
    "        TODO:\n",
    "        * Parallelise?\n",
    "        \"\"\"\n",
    "        self.forecasts = f\n",
    "        self.observations = o \n",
    "        \n",
    "    def compute_BS(self,):\n",
    "        return 0\n",
    "    \n",
    "    def compute_BSS(self,):\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deploy Lorenz's 1963 three-variable model of convection (L63) in an intermittent regime (i.e., extreme events come in infrequent bursts). We will create two ensembles: one with more accurate initial conditions than the other to simulate a \"good\" and \"bad\" model. We will also consider two different levels of exceedence in the L63 z-variable time series to represent an event occurring. The higher percentile chosen represents the rarer event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double approx help from github user kendixon\n",
    "\n",
    "class L63:\n",
    "    def __init__(self,x0,y0,z0,rho,\n",
    "                 sigma=10.0,beta=2.667,dt=1e-3):\n",
    "        \"\"\"Chop off spin-up time?\n",
    "        \n",
    "        TODO:\n",
    "            * Remove spin-up time (transients)\n",
    "            * Add perturbations (with random seed allowed as arg)\n",
    "        the paper values use the intermittent regime\n",
    "        \n",
    "        rho\n",
    "        166.08 is intermittent\n",
    "        166.1 is the rarer variation\n",
    "        28.0 is the lorenz default\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.rho = rho\n",
    "        self.beta = beta\n",
    "        self.dt =dt\n",
    "        self.t = 0.0\n",
    "\n",
    "        # The data array in x,y,z. Time appended in axis=3\n",
    "        self.output = np.zeros((3,1))\n",
    "        self.output[:,0] = [x0,y0,z0]\n",
    "        # self.\n",
    "\n",
    "    def dxdt(self,x,y):\n",
    "        return self.sigma * (y-x)\n",
    "    def dydt(self,x,y,z):\n",
    "        return x * (self.rho - z) - y\n",
    "    def dzdt(self,x,y,z):\n",
    "        return (x*y) - (self.beta * z)\n",
    "    \n",
    "    def double_approx(self,x,y,z):\n",
    "        dx1 = self.dxdt(x,y)*self.dt + x\n",
    "        dy1 = self.dydt(x,y,z)*self.dt + y\n",
    "        dz1 = self.dzdt(x,y,z)*self.dt + z\n",
    "        \n",
    "        dx2 = self.dxdt(dx1,dy1)\n",
    "        dy2 = self.dydt(dx1,dy1,dz1)\n",
    "        dz2 = self.dzdt(dx1,dy1,dz1)\n",
    "        \n",
    "        x2 = self.dt*(0.5*dx1 + 0.5*dx2) + x\n",
    "        y2 = self.dt*(0.5*dy1 + 0.5*dy2) + y\n",
    "        z2 = self.dt*(0.5*dz1 + 0.5*dz2) + z\n",
    "        \n",
    "        return x2,y2,z2\n",
    "        \n",
    "    def integrate_once(self,):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t0    : Initial time\n",
    "            y0    : Intiial state\n",
    "        \"\"\"\n",
    "        x = self.output[0,-1]\n",
    "        y = self.output[1,-1]\n",
    "        z = self.output[2,-1]\n",
    "        # print(x,y,z)\n",
    "        ret = self.double_approx(x,y,z)\n",
    "        # print(ret)\n",
    "        new_time = np.expand_dims(np.array(ret),axis=1)\n",
    "        return new_time\n",
    "        \n",
    "    def integrate(self,n,clip=None):\n",
    "        for n in range(n):\n",
    "            next_data = self.integrate_once()\n",
    "            # print(self.output.shape,next_data.shape)\n",
    "            self.output = np.concatenate((self.output,next_data),axis=1)\n",
    "            # print(self.output)\n",
    "        #print(self.output)\n",
    "        if clip is not None:\n",
    "            self.output = self.output[:,clip:]\n",
    "            # cut_nt = nt - clip\n",
    "        return \n",
    "\n",
    "    def get_exceedence_ts(self,pc,vrbl='z'):\n",
    "        vdata = self.output[: ,2]\n",
    "        exceed = self.get_pc_exceed(vdata,pc)\n",
    "        return exceed\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_pc_exceed(ts,pc):\n",
    "        return ts > np.percentile(ts,pc)\n",
    "    \n",
    "    @staticmethod\n",
    "    def do_windowing(ts,wsize):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ts    : time series of raw data.\n",
    "        wsize : epoch window size in time steps\n",
    "        \"\"\"\n",
    "        nt = ts.size\n",
    "        epochs = np.zeros(int(nt/wsize)+1).astype(bool)\n",
    "        widx = np.arange(0,nt,wsize)\n",
    "        windows = np.zeros_like(ts).astype(bool)\n",
    "        epoch_idx = np.arange(int(wsize/2),nt,wsize)\n",
    "        for nc,cidx in enumerate(widx):\n",
    "            idx0 = int(nc*wsize)\n",
    "            idx1 = int(idx0 + wsize)\n",
    "            this_window = ts[idx0:idx1]\n",
    "            epochs[nc] = True in this_window\n",
    "            windows[idx0:idx1] = bool(epochs[nc])    \n",
    "        return windows        \n",
    "    \n",
    "    @classmethod\n",
    "    def mask_and_window(cls,arr,pc,wsize):\n",
    "        if arr.ndim == 1:\n",
    "            nt = arr.size\n",
    "            arr = arr.reshape(1,nt)\n",
    "        pc_arr = cls.get_pc_exceed(arr,pc)\n",
    "        window_ts = np.zeros_like(pc_arr)\n",
    "        for ne in range(pc_arr.shape[0]):\n",
    "            window_ts[ne,:] = cls.do_windowing(pc_arr[ne,:],wsize)\n",
    "        # window_ts = cls.do_windowing(pc_ts,wsize)\n",
    "        return window_ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the z-component (in L63, this represents rising motion in convective cells) as our time series. We convert to a binary forecast by identifying where the time series exceeds the given percentile. We then quantise in time to both introduce tolerance and mimic EFS real-world output.\n",
    "\n",
    "A cool thought: the rarer the event, the more that average surprise (shannon entropy) is distributed at extreme bipoles. Hence some of the apparent contradiction that uncertainty around a common event (e.g., cloudy) that has most entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO look at old checkpoints\n",
    "### def generate_ensemble\n",
    "\n",
    "def generate_truth(rho,pcs,chunksize,nt_all,cutoff):\n",
    "    lorenz = L63(x0,y0,z0,rho=rho)\n",
    "    lorenz.integrate(nt_all,clip=cutoff)\n",
    "    TRUTHS = {rho:{}}\n",
    "    for pc in pcs:\n",
    "        TRUTHS[rho][pc] = lorenz.get_exceedence_chunks(\n",
    "                pc,chunksize)[::chunksize]\n",
    "    return TRUTHS\n",
    "\n",
    "def generate_ensemble(nt,tweak_max,Ne,chunksize,pcs,x0,y0,z0,rho):\n",
    "    nchunks = 1 + int(nt/chunksize)\n",
    "\n",
    "    # Save generated forecasts from each\n",
    "    ENSEMBLES = {rho:{pc:N.zeros([Ne,nchunks]) for pc in pcs}}\n",
    "    tweaks = N.random.uniform(0,tweak_max,Ne)\n",
    "    for ne, tweak in enumerate(tweaks):\n",
    "        lorenz = L63(x0=x0+tweak,y0=y0+tweak,z0=z0+tweak,\n",
    "                         rho=rho)\n",
    "        lorenz.integrate(nt_all,clip=cutoff)\n",
    "        for pc in pcs:\n",
    "            ENSEMBLES[rho][pc][ne,:] = lorenz.get_exceedence_chunks(pc,\n",
    "                                    chunksize)[::chunksize]\n",
    "        print(f\"Computing member {ne}\")\n",
    "  \n",
    "    return ENSEMBLES\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-35202acf68ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mOBS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnt_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mOBS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_truth' is not defined"
     ]
    }
   ],
   "source": [
    "# Want truth to be window size 100?\n",
    "# Then test with 200,500 window size\n",
    "\n",
    "# Catalogue of runs\n",
    "# first idx is obs, rest is fcst \n",
    "\n",
    "\n",
    "# Do numerous \"days\" for a longer experiment?\n",
    "\n",
    "# Parallelisation?\n",
    "# Initial conditions\n",
    "x0 = 1.0\n",
    "y0 = 1.0\n",
    "z0 = 1.0\n",
    "\n",
    "# Total integration steps\n",
    "nt_all = 25000\n",
    "\n",
    "# Spin-up period to be removed\n",
    "cutoff = 15000\n",
    "\n",
    "# Window size for scale-aware\n",
    "# Can do later when computing scores\n",
    "# wsize = 500 \n",
    "\n",
    "# Number of ensemble members\n",
    "# Can subset later\n",
    "Ne = 200\n",
    "\n",
    "# Number of time steps in raw data after spin-up is cut off\n",
    "nt = nt_all - cutoff\n",
    "\n",
    "# Measure of intermittency\n",
    "rho_list = [166.08,166.09,166.1]\n",
    "# percentiles for thresholding\n",
    "pc_list = [94.0,97.0,99.5]\n",
    "# IC perturbations for ensembles\n",
    "tweak_max_list = [5e-7,1e-6] \n",
    "\n",
    "# wsize = 100 will be minimum (i.e. truth)\n",
    "wsize_list = [100,200,500,1000]\n",
    "\n",
    "# Raw\n",
    "OBS = {r:{} for r in rho_list}\n",
    "FCST = {r:{t:{} for t in tweak_max_list} for r in rho_list}\n",
    "\n",
    "# Windowed and percentile-masked\n",
    "OBS_PW = {r:{pc:{w:0 for w in wsize_list} for pc in pc_list} \n",
    "                for r in rho_list}\n",
    "FCST_PW = {r:{t:{pc:{w:0 for w in wsize_list} for pc in pc_list}\n",
    "                for t in tweak_max_list} for r in rho_list}\n",
    "    \n",
    "for rho, tweak_max in itertools.product(rho_list,tweak_max_list):\n",
    "    obs_file = f'data/obs_{int(rho*1000)}.pickle'\n",
    "    fcst_file = f'data/fcst_{int(rho*1000)}_{int(tweak_max*1e7)}.pickle'\n",
    "\n",
    "    # Create observation data (only once per IC perturbation value (tweak_max))\n",
    "    if tweak_max == tweak_max_list[0]:\n",
    "        if os.path.exists(obs_file):\n",
    "            with open(obs_file,'rb') as p:\n",
    "                OBS[rho] = pickle.load(p)\n",
    "        else:\n",
    "            obs = generate_truth(x0,y0,z0,rho,nt_all,cutoff)\n",
    "            OBS[rho] = obs\n",
    "            with open(obs_file,'wb') as p:\n",
    "                pickle.dump(obs,p)\n",
    "\n",
    "        for pc, wsize in itertools.product(pc_list,wsize_list):\n",
    "            ts_pw = L63.mask_and_window(OBS[rho],pc,wsize)\n",
    "            OBS_PW[rho][pc][wsize] = ts_pw\n",
    "    \n",
    "    # Create forecast data\n",
    "    if os.path.exists(fcst_file):\n",
    "        with open(fcst_file,'rb') as p:\n",
    "            FCST[rho][tweak_max] = pickle.load(p)\n",
    "    else:\n",
    "        xp = x0 + np.random.uniform(-tweak_max,tweak_max)        \n",
    "        yp = y0 + np.random.uniform(-tweak_max,tweak_max)\n",
    "        zp = z0 + np.random.uniform(-tweak_max,tweak_max)\n",
    "\n",
    "        fcst = generate_ensemble(xp,yp,zp,rho,nt_all,cutoff,tweak_max,Ne)\n",
    "        FCST[rho][tweak_max] = fcst\n",
    "        with open(fcst_file,'wb') as p:\n",
    "            pickle.dump(fcst,p)\n",
    "            \n",
    "    for pc, wsize in itertools.product(pc_list,wsize_list):\n",
    "        ts_pw = L63.mask_and_window(FCST[rho][tweak_max],pc,wsize)\n",
    "        FCST_PW[rho][tweak_max][pc][wsize] = ts_pw\n",
    "        \n",
    "# [Ne x Nt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(raw_obs,obs_pw,raw_fcst,pc,fcst_pw,wsize,title):\n",
    "    # Plotting\n",
    "    fig,axes = plt.subplots(figsize=(7,7),ncols=1,nrows=4)\n",
    "\n",
    "    nt = raw_obs.size\n",
    "    raw_xlocs = np.arange(nt)\n",
    "\n",
    "    axes.flat[0].plot(raw_xlocs,raw_obs,color='red')\n",
    "    axes.flat[0].set_title(title)\n",
    "    axes.flat[0].hlines(y=np.percentile(raw_obs,pc),\n",
    "                           xmin=0,xmax=nt,color='k')\n",
    "\n",
    "    \n",
    "    # axes.flat[1].fill(raw_xlocs,obs_pw,color='m')\n",
    "    axes.flat[1].plot(raw_xlocs,obs_pw,color='m')\n",
    "    axes.flat[2].plot(raw_xlocs,raw_fcst,color='red')\n",
    "    axes.flat[2].hlines(y=np.percentile(raw_fcst,pc),\n",
    "                           xmin=0,xmax=nt,color='k')\n",
    "        \n",
    "#    window_locs = np.arange(1,nt+wsize,wsize)\n",
    "#    axes.flat[3].fill_between(x=window_locs,y1=fcst_pw[::wsize],\n",
    "#                                y2=0,step='pre',color='r')\n",
    "#    axes.flat[3].vlines(x=window_locs,ymin=0,ymax=1,color='k')\n",
    "    axes.flat[3].fill(raw_xlocs,fcst_pw,color='k')   \n",
    "    # make prettier\n",
    "    for n in (1,3):\n",
    "        axes.flat[n].hlines(y=(0,1),xmin=0,xmax=nt,color='k')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 166.08\n",
    "pc = 97.0\n",
    "wsize = 500\n",
    "tweak_max = 5e-7\n",
    "raw_fcst = FCST[rho][tweak_max][0,:]\n",
    "fcst_pw_data = FCST_PW[rho][tweak_max][pc][wsize][0,:]\n",
    "obs_data = OBS[rho]\n",
    "obs_pw_data = OBS_PW[rho][pc][wsize][0,:]\n",
    "\n",
    "plot_example(raw_obs=obs_data,obs_pw=obs_pw_data,raw_fcst=raw_fcst,\n",
    "             pc=pc,fcst_pw=fcst_pw_data,wsize=wsize,title=rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Sinai Entropy\n",
    "We can now estimate the information provided by the EFS. But what is the prior uncertainty of the atmosphere? We can measure this in terms of __inforamtion loss per unit time__ for a model $\\mathcal{M}$ with Kolmogorov-Sinai Entropy $H_{KS}$:\n",
    "\n",
    "$$ H_{KS}(\\mathcal{M}) =  \\tau^{-1}\\sum^{N_r}_{r=0} p_r \\log_2 {p_r}^{-1} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ p_r = \\prod^{{N_\\alpha}}_{\\alpha=0} p_{\\alpha} $$\n",
    "\n",
    "Above, $p_r$ represents the transition probabilies for each route along the decision tree (schematic of decision trees).\n",
    "\n",
    "As $N_\\alpha$ increases, there is more chance of encounting a rare event; as a rare event is exponentially rewarded, the $H_{\\times}$ penalty is \"bursty\" also. Hence we estimate the limiting value by computing $H_{KS}$ over 4--20 epochs (output times; in contrast to real-world time $\\tau$). As we do not have access to the atmosphere's rate of information loss, this is estimated from a prior EFS. Of course, this is not ideal due to cyclical logic, but we assume a perfect model providing prior probabilies. (This could stand for a 36-hour forecast such as HREF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_TPs(N_epoch,ensemble,debug=False):\n",
    "    \"\"\"\n",
    "    debug : 0 is off, 1 is some, 2 is all, 3 is pauses\n",
    "    \"\"\"\n",
    "    N_routes = 2**N_epoch\n",
    "    TPs = np.zeros([N_epoch,N_routes],dtype=float)\n",
    "\n",
    "    # epoch 0: \n",
    "    epoch_data = ensemble[:,0].astype(bool)\n",
    "    e0_true_idx = np.flatnonzero(epoch_data)\n",
    "    e0_false_idx = np.flatnonzero(~epoch_data)\n",
    "\n",
    "    # node 0:\n",
    "    # Save which members were in which set\n",
    "    nodeidx = {e:{False:[],True:[]} for e in range(N_epoch)}\n",
    "    # Create lists of lists: [[idx_node0],[idx_node1],etc...]\n",
    "    nodeidx[0][False].append(e0_false_idx)\n",
    "    nodeidx[0][True].append(e0_true_idx)\n",
    "\n",
    "    # transition 0:\n",
    "    hlf = int(N_routes/2)\n",
    "    TPs[0,:hlf] = e0_false_idx.size/Ne\n",
    "    TPs[0,hlf:] = e0_true_idx.size/Ne\n",
    "\n",
    "    print(\"+\"*9,f\"Computing K-S Entropy for {N_epoch}\"\n",
    "            f\" epochs, for {Ne} members.\")\n",
    "\n",
    "    # Now loop over epochs to generate TPs\n",
    "    for epoch in range(1,N_epoch):\n",
    "        if debug==1:\n",
    "            print(f\"\\n >>>>>>>>> For epoch {epoch}:\")\n",
    "\n",
    "        # Number of nodes\n",
    "        nnodes = int((2**(epoch+1))/2)\n",
    "        nodes = np.arange(nnodes,dtype=int)\n",
    "\n",
    "        # Number of True/False groups (2x node)\n",
    "        nsets = int(2**(epoch+1))\n",
    "        # Indices to put for each node \n",
    "        sets = np.array([((n*2),(n*2)+1,(n*2)+2)\n",
    "                       for n in nodes]).astype(int)\n",
    "\n",
    "        # Indices for placing TPs in array\n",
    "        tp_idx = np.linspace(0,N_routes,num=nsets+1,dtype=int)\n",
    "\n",
    "        # node 0, 1, 2 etc...\n",
    "        for node,_sets in zip(nodes,sets):\n",
    "            if debug: print(\"Node:\",node)\n",
    "            prev_node = int(np.floor(node/2))\n",
    "            # Get members that were true and false in the last epoch\n",
    "            if node%2 == 0:\n",
    "                mems = nodeidx[epoch-1][False][prev_node]\n",
    "            else:\n",
    "                mems = nodeidx[epoch-1][True][prev_node] # These 0 idx needs to be a prev node?\n",
    "            if debug: print(\"Node members:\",mems)\n",
    "\n",
    "            # Get probability for each group moving to true/false\n",
    "            bools = ensemble[mems,epoch].astype(bool)\n",
    "            e_true_idx = np.flatnonzero(bools)\n",
    "            e_false_idx = np.flatnonzero(~bools)\n",
    "\n",
    "            e_true_mems = mems[e_true_idx]\n",
    "            e_false_mems = mems[e_false_idx]\n",
    "\n",
    "            if debug: \n",
    "                print(e_false_idx,e_true_idx)\n",
    "                print(e_false_mems,e_true_mems)\n",
    "\n",
    "            false_frac = np.divide(e_false_idx.size,mems.size) if mems.size != 0 else 0.0\n",
    "            true_frac = np.divide(e_true_idx.size,mems.size) if mems.size != 0 else 0.0\n",
    "\n",
    "            idxs = dict()\n",
    "            for n in range(3):\n",
    "                idxs[n] = tp_idx[_sets[n]]\n",
    "            TPs[epoch,idxs[0]:idxs[1]] = false_frac\n",
    "            TPs[epoch,idxs[1]:idxs[2]] = true_frac\n",
    "            if debug: print(false_frac,true_frac)\n",
    "            if (e_false_idx.size!=0) and (e_true_idx.size!=0):\n",
    "                assert np.isclose(false_frac,1-true_frac)\n",
    "\n",
    "            if debug: print(\"-\"*20,f\"PROB = {true_frac:0.2f}\",\"-\"*20)\n",
    "\n",
    "            # Now save the members from this set \n",
    "            nodeidx[epoch][False].append(e_false_mems)\n",
    "            nodeidx[epoch][True].append(e_true_mems)\n",
    "            if debug:\n",
    "                print(\"True members are:\",e_true_mems)\n",
    "                print(\"False members are:\",e_false_mems)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "    return TPs\n",
    "\n",
    "def get_KSE(N_epoch,ensemble,debug=0):\n",
    "    TPs = get_TPs(N_epoch,ensemble=ensemble)\n",
    "    route_probs_all = np.prod(TPs,axis=0)\n",
    "    route_probs = route_probs_all[route_probs_all > 0]\n",
    "    indiv_H = np.nan_to_num(route_probs * np.log2(1/route_probs))\n",
    "    total_H = np.sum(indiv_H)\n",
    "    KSE = total_H/N_epoch\n",
    "    return KSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute H_{KS} for the more common event $\\xi_{93}$ and rarer event $\\xi_{99}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PC1 - more common event\n",
    "\n",
    "# Show probs for each epoch\n",
    "def print_KSE(ensemble,do_probs=True):\n",
    "    if do_probs:\n",
    "        prob_ts = np.mean(ensemble,axis=0)\n",
    "        fig,axes = plt.subplots(ncols=1,nrows=2)\n",
    "        axes.flat[0].plot(prob_ts,color='g')\n",
    "        axes.flat[0].set_ylim([0,1])\n",
    "    N_epochs = np.arange(4,21,1)\n",
    "    KSE = dict()\n",
    "    for N_epoch in N_epochs:\n",
    "        KSE[N_epoch] = get_KSE(N_epoch=N_epoch,\n",
    "                        ensemble=ensemble,debug=0)    \n",
    "    \n",
    "        print(f\"KSE for {N_epoch} epochs:\"\n",
    "                  f\"{KSE[N_epoch]:0.3f}\")\n",
    "        axes.flat[1].scatter(N_epoch,KSE[N_epoch],\n",
    "                        marker=\"^\",c=\"m\")\n",
    "    axes.flat[1].set_ylim([0,max(KSE.values())])\n",
    "    \n",
    "print_KSE(ENSEMBLES[rho][pcs[0]])\n",
    "# DO DIFFERENCE PLOTS LIKE WILLIAMS BOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper plot shows the raw probabilities (unbounded?). The lower plot shows H_{KS} as a function of epoch length $N_\\alpha$: it appears to converge to $\\sim 0.25$ bits per epoch. In other (abstract) terms, the system is generating uncertainty at a rate of a coin-flip's worth every four output times. But of course the average (which $H_{KS}$ measures) hides the intermittency of this information loss (which is tied up in the poorly forecast occurence of rare events). Let's compute $H_{KS}$ for the rarer event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PC2  - rarer event\n",
    "# Now we show what a rarer event looks like\n",
    "\n",
    "print_KSE(ENSEMBLES[rho][pcs[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the upper panel, we see this event is indeed much rarer. However, the bottom panel suggests information loss is similar to the more common event, in the limit of $N_\\alpha$. This is because the system generating the time series is the same.\n",
    "\n",
    "Now, let's recompute a \"good\" and \"bad\" ensemble, this time running L63 in a more intermittent regime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 166.1\n",
    "\n",
    "_TRUTHS, _ENSEMBLES = generate_all_L63(x0,y0,z0,rho,cutoff,nt_all,\n",
    "                            chunksize,pcs,plot_both=True,\n",
    "                            title=f\"x0,y0,z0 as {x0}\",tweak_max=tweak_max)\n",
    "\n",
    "TRUTHS.update(_TRUTHS)\n",
    "ENSEMBLES.update(_ENSEMBLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_KSE(ENSEMBLES[rho][pcs[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_KSE(ENSEMBLES[rho][pcs[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, despite different probability time series, the value of $H_{KS}$ converges to a similar value. With a smaller value of $N_\\alpha$, the value of $H_{KS}$ is more sensitive to individual events. The forecaster is likely more interested in short periods of large surprise rather than the average surprise of common events, so averaging hides the real-world impact.\n",
    "\n",
    "The reason why the more predictable situation has a higher H_KS at the start is because you are more likely to hit a bad forecast (close to 0.5, max entropy). For the less predictable situation, you need a longer time series to demonstrate larger surprise. Eventually it converges to the same value because it's the same system, despite reducing to a binary (percentile exceedence).\n",
    "\n",
    "Now we have estimated prior uncertainty, let's evaluate the two EFS forecasts for each regime (less and more intermittent), using the unperturbed L63 runs as observed \"truth\" with no error. Note we bound forecast probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed random generator for repeatability!\n",
    "# np.random.seed(1)\n",
    "def get_thresh(Ne):\n",
    "    thresh = (1/(3*Ne))\n",
    "    return thresh\n",
    "\n",
    "def bound_low(prob,limit=0.0001):\n",
    "    return max(limit,prob)\n",
    "\n",
    "def bound_high(prob,limit=0.9999):\n",
    "    return min(limit,prob)\n",
    "\n",
    "def bound(prob):\n",
    "    if prob < 0.5:\n",
    "        return bound_low(prob)\n",
    "    else:\n",
    "        return bound_high(prob)\n",
    "\n",
    "def bound_ensemble(probs,Ne):\n",
    "    thresh = get_thresh(Ne)\n",
    "    probs[probs<thresh] = thresh\n",
    "    probs[probs>(1-thresh)] = 1-thresh\n",
    "    return probs\n",
    "    \n",
    "def remove_nans(val):\n",
    "    if isinstance(val,np.ndarray):\n",
    "        val[np.isnan(val)] = 0\n",
    "        val[val == -np.inf] = 0\n",
    "        val[val == -np.inf] = 0\n",
    "    elif isinstance(val,float):\n",
    "        if np.isnan(val) or val==np.inf or val==-np.inf:\n",
    "            val = 0\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy - find in info gain script\n",
    "\n",
    "# Maybe work out BS to compare the reward of rare events - histogram of scores per epoch?\n",
    "\n",
    "def compute_DKL(f,o,):\n",
    "    rn = remove_nans\n",
    "    DKL = rn((1-o)*np.log2((1-o)/(1-f))) + rn(o*np.log2(o/f))\n",
    "    #pdb.set_trace()\n",
    "    return remove_nans(DKL)\n",
    "\n",
    "def compute_DSC(nk,ok,o):\n",
    "    ok = remove_nans(ok)\n",
    "    p_y = nk/np.sum(nk)\n",
    "    p_o = np.mean(o)\n",
    "    # dsc_all = p_y * ((ok * np.log2(ok/o)) + ((1-ok) * np.log2((1-ok)/(1-o)))) \n",
    "    dsc_all = p_y * (remove_nans(ok * np.log2(ok/p_o)) + \n",
    "                        remove_nans((1-ok) * np.log2((1-ok)/(1-p_o))) ) \n",
    "    # val_allk = nk*( (ok * np.log2(ok/o)) + ( (1-ok) * np.log2((1-ok)/(1-o)) ) )\n",
    "    # val = np.nansum(val_allk)/np.nansum(nk)\n",
    "    # val = np.sum(val_allk)/np.sum(nk)\n",
    "    return np.sum(dsc_all)\n",
    "\n",
    "def compute_REL(nk,ok,fk):\n",
    "    \"\"\"\n",
    "    nk is number of forecasts in this prob bin, 2D\n",
    "    ok_arr has value of o-bar for each prob bin: 2D\n",
    "    fk_arr has prob value for each bin: 2D\n",
    "    likewise, iok and ifk are for the not-observed values.\n",
    "    \"\"\"\n",
    "    ok = remove_nans(ok) \n",
    "    p_y = nk/np.sum(nk)\n",
    "    rel_all = p_y * (\n",
    "        remove_nans(ok * np.log2(ok/fk)) + \n",
    "        remove_nans((1-ok) * np.log2((1-ok)/(1-fk))) ) \n",
    "    return np.sum(rel_all)\n",
    "    # return remove_nans(val)\n",
    "\n",
    "def compute_UNC(o):\n",
    "    p_o = np.mean(o)\n",
    "    unc = -p_o*np.log2(p_o) - (1-p_o)*np.log2(1-p_o)\n",
    "    return unc \n",
    "    \n",
    "def compute_skill_score(unc,dkl=None,rel=None,dsc=None):\n",
    "    if dkl is None:\n",
    "        pass\n",
    "    else:\n",
    "        val = (np.mean(dkl)-unc)/(0-unc)\n",
    "    return val\n",
    "    \n",
    "def S_per_epoch():\n",
    "    # Surprise per epoch\n",
    "    # Can ignore penalty for non-unity prob of correct negatives\n",
    "    \n",
    "    # if ob = 0\n",
    "    # only punish for false alarm component\n",
    "    \n",
    "    # if ob = 1\n",
    "    # punish for missed hit and correct positive error\n",
    "    \n",
    "    pass\n",
    "    \n",
    "# want to plot and compute per epoch (like object based)\n",
    "# and also for the whole period. And discuss skill score. \n",
    "# The choice of UNC could be set depending on the inherent unc.\n",
    "\n",
    "# reorganise so there is a clear collection of ensembles/truth\n",
    "\n",
    "# then can vary different things and show the results below\n",
    "# 20 v 40 ensemble members\n",
    "# two regimes of L63 and different percentiles\n",
    "# window length (chunk size) -> rename to window?\n",
    "# dt of L63\n",
    "\n",
    "\n",
    "def get_fk(Ne):\n",
    "    pp = np.linspace(0,1,Ne+1)\n",
    "    fk = bound_ensemble(pp,Ne) \n",
    "    return fk\n",
    "        \n",
    "def compute_surprise(f,o,fk):\n",
    "    nk = np.zeros_like(fk)\n",
    "    ok = np.zeros_like(fk)\n",
    "    DKL = np.ones(N_epoch)\n",
    "    oc = 0\n",
    "    for epoch in np.arange(N_epoch):\n",
    "        pbin_idx = np.where(fk==f[epoch])[0][0]\n",
    "        nk[pbin_idx] += 1\n",
    "        ok[pbin_idx] += o[epoch]\n",
    "        oc += o[epoch]\n",
    "        DKL[epoch] = compute_DKL(f[epoch],o[epoch])\n",
    "    return DKL, nk, ok,oc\n",
    "    \n",
    "def compute_o_freq(f,o,fk,N_epoch):\n",
    "    oo = o.flatten()\n",
    "    ff = f.flatten()\n",
    "    nk = np.zeros_like(fk)\n",
    "    ok = np.zeros_like(fk)\n",
    "\n",
    "    for kidx,k in enumerate(fk):\n",
    "        kwhere = np.where(ff==k)\n",
    "        nk[kidx] = kwhere[0].shape[0]\n",
    "        o_ct = oo[kwhere]\n",
    "        ok[kidx] = np.sum(o_ct)/N_epoch\n",
    "    return nk, ok\n",
    "    \n",
    "def do_verif_suite(f,o,fk,N_epoch,DKL_only=False):\n",
    "    DKL = compute_DKL(f,o)\n",
    "    if DKL_only:\n",
    "        return DKL\n",
    "    UNC = compute_UNC(o)\n",
    "    nk,ok = compute_o_freq(f,o,fk,N_epoch)\n",
    "    DSC = compute_DSC(nk,ok,o)\n",
    "    REL = compute_REL(nk,ok,fk)\n",
    "    # SS = compute_skill_score(dsc=DSC,rel=REL,unc=UNC)\n",
    "    SS = compute_skill_score(unc=UNC,dkl=DKL)\n",
    "    return DSC, REL, UNC, DKL, SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pdb on\n",
    "\n",
    "rhos = (166.08,166.1)\n",
    "\n",
    "def do_DKL(ens,rhos,pcs,subsample=None):\n",
    "    N_epoch = 20\n",
    "    # Get probs for all ensembles\n",
    "    # pcs = [91.0,99.5]\n",
    "    Ne = ens[rhos[0]][pcs[0]].shape[0]\n",
    "    if subsample is not None:\n",
    "        Ne = int(subsample * Ne)\n",
    "    PROBS = {rho:{pc:-1*np.ones([N_epoch]) for pc in pcs} \n",
    "                 for rho in rhos}\n",
    "    DKL = {rho:{pc:0 for pc in pcs}for rho in rhos}\n",
    "    DSC = {rho:{pc:0 for pc in pcs}for rho in rhos}\n",
    "    REL = {rho:{pc:0 for pc in pcs}for rho in rhos}\n",
    "    UNC = {rho:{pc:0 for pc in pcs}for rho in rhos}\n",
    "    SS = {rho:{pc:0 for pc in pcs}for rho in rhos}\n",
    "    for rho, pc in itertools.product(rhos,pcs):\n",
    "        PROBS[rho][pc] = np.mean(ens[rho][pc][:Ne,:],axis=0)\n",
    "        f = bound_ensemble(PROBS[rho][pc],Ne)\n",
    "        fk = get_fk(Ne)\n",
    "        # nk = get_nk()\n",
    "        o = TRUTHS[rho][pc].astype(int)\n",
    "        returns = do_verif_suite(f,o,fk,N_epoch)\n",
    "        DSC[rho][pc] = returns[0]\n",
    "        REL[rho][pc] = returns[1]\n",
    "        UNC[rho][pc] = returns[2]\n",
    "        DKL[rho][pc] = returns[3]\n",
    "        SS[rho][pc] = returns[4]\n",
    "\n",
    "    return DSC, REL, UNC, DKL, SS\n",
    "\n",
    "DSC,REL,UNC,DKL,SS = do_DKL(ENSEMBLES,rhos = [166.08,166.1],pcs=pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DSC)\n",
    "print(REL)\n",
    "print(UNC)\n",
    "print(DKL)\n",
    "print(SS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "%pdb off\n",
    "def plot_bars(DKL,rhos,pcs,N_epoch=20):\n",
    "    def do_bar(ax,DKL,rho,pc,c,N_epoch,side,width=0.6):\n",
    "        if side == 'left':\n",
    "            wd = -width/2\n",
    "        else:\n",
    "            wd = width/2\n",
    "        xx = np.arange(N_epoch+1) + wd\n",
    "        ax.bar(xx,DKL[rho][pc],wd,\n",
    "                label=f\"rho = {rho:.2f}, pc = {pc:.1f}\",color=c)\n",
    "        ax.hlines(xmin=0,xmax=N_epoch,y=np.mean(DKL[rho][pc]),color=c)\n",
    "        return ax\n",
    "\n",
    "    fig,axes = plt.subplots(ncols=1,nrows=len(rhos),\n",
    "                                figsize=(10,5*len(rhos)))\n",
    "    cols = iter(['r','m','g','b'])\n",
    "    nt = nt_all - cutoff\n",
    "\n",
    "    for rho, pc in itertools.product(rhos,pcs):\n",
    "        c = next(cols)\n",
    "        if len(rhos) == 1:\n",
    "            ax = axes\n",
    "        elif rho == rhos[0]:\n",
    "            ax = axes.flat[0]\n",
    "        elif rho == rhos[1]:\n",
    "            ax = axes.flat[1]\n",
    "\n",
    "        if pc == pcs[0]:\n",
    "            side = 'left'\n",
    "        else:\n",
    "            side = 'right'\n",
    "        ax = do_bar(ax,DKL,rho,pc,c,N_epoch,side)\n",
    "    \n",
    "    if len(rhos) == 1:\n",
    "        axes.legend()\n",
    "    else:\n",
    "        for nrho in range(len(rhos)):\n",
    "            axes.flat[nrho].legend()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(DKL,rhos,pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bDSC,bREL,bUNC,bDKL,bSS = do_DKL(bad_ENSEMBLES,rhos = [166.08,],pcs=pcs)\n",
    "print(bDSC)\n",
    "print(bREL)\n",
    "print(bUNC)\n",
    "print(bDKL)\n",
    "print(bSS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(bDKL,[166.08,],pcs=pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run an ensemble with third of the members \n",
    "rho = 166.08\n",
    "# Ne_half = int(Ne/3)\n",
    "# dkl_half = {q:{rho:{pc:0 for pc in pcs}for rho in rhos} for\n",
    "#                 q in (\"good\",\"bad\")}\n",
    "\n",
    "# for ENS, q in zip((ENSEMBLES,bad_ENSEMBLES),(\"good\",\"bad\")):\n",
    "#     bDSC,bREL,bUNC,bDKL,bSS = do_DKL(bad_ENSEMBLES,rhos = [166.08,],pcs)\n",
    "\n",
    "    \n",
    "    #for pc in pcs:\n",
    "    #    p_half = np.mean(ENS[rho][pc][:Ne_half,:],axis=0)\n",
    "    #    f_half = bound_ensemble(p_half,Ne_half)\n",
    "    #    fk_half = get_fk(Ne_half)\n",
    "        # nk = get_nk()\n",
    "    #    o = TRUTHS[rho][pc].astype(int)\n",
    "    #    _1, _2, _3, dkl_half[q][rho][pc], _4 = do_verif_suite(\n",
    "    #                f_half,o,fk_half)\n",
    "    \n",
    "sDSC,sREL,sUNC,sDKL,sSS = do_DKL(ENSEMBLES,\n",
    "                        rhos = [166.08,],pcs=pcs,subsample=0.33)\n",
    "  \n",
    "    \n",
    "print(sDSC)\n",
    "print(sREL)\n",
    "print(sUNC)\n",
    "print(sDKL)\n",
    "print(sSS)\n",
    "plot_bars(sDKL,[rho,],pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbDSC,sbREL,sbUNC,sbDKL,sbSS = do_DKL(bad_ENSEMBLES,\n",
    "                        rhos = [166.08,],pcs=pcs,subsample=0.33)    \n",
    "    \n",
    "print(sbDSC)\n",
    "print(sbREL)\n",
    "print(sbUNC)\n",
    "print(sbDKL)\n",
    "print(sbSS)\n",
    "plot_bars(sbDKL,[rho,],pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\U0001F923\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1) Thunderstorms, 3km, 100 members\n",
    "Intuitively, the end-user should act when $\\mathrm{P_f}(\\xi) > \\frac{C(\\xi)}{L(\\xi)}$, if the model is reliable (good assumption?), and assuming we only want to prevent a mean loss over a long time period. The change in skill of the model is the remaining cross-entropy in the forecast:\n",
    "\n",
    "$$ H_\\times = D_{KL} + H \\\\\n",
    "            = RES_f -+? REL_f + UNC_f + H(o) $$\n",
    "\n",
    "where we assume that doubling the ensemble members only improves the REL and degrades the RES (though this isn't always the case, like AI paper about compression). We would like to know how much better the reliability must be (normalised by the cost per ...).\n",
    "\n",
    "Again, intuitively, the probabilities make a bigger difference to Cath, as the loss accrued by not mitigating a hazard is much smaller for Igor. Further, thunderstorms are uncommon, meaning that average surprise is low (it is usually safe to assume there are no thunderstorms), but the event surprise is high.\n",
    "\n",
    "(Do we need more ensemble members with a higher resolution? Can we show that in maths?)\n",
    "\n",
    "There is also the problem that, if an event is too rare, that it will not be sampled by the ensemble, leading to a catastrophic failure. Bounding the minimum probability (e.g., 1%) is a safeguard, but intractable to the end user for very rare events (it would either be interpreted as 0%, or C/L would be higher than the hazard frequency, leading to a net loss). This shows why forecasts should not be issued beyond an information horizon - this asymptotes, therefore a tolerance must be introduced whereby returns have diminished compared to the cost of running to that time. This is related to CC, but also for rarer events, because the small skill is multiplied by value, and also by rarity of the event. When time becomes involved, we need the K-S entropy. (A rarer event has more entropy. Also, a smaller spatial scale has lower predictability and faster error growth, and more information is needed per square km (?), but show as percent of ignorance removed, or absolute information gained in bits.)\n",
    "\n",
    "Can we show info gain per unit resource? Or how much the REL v RES must improve to decide how to spend the resources. Information is equally as useful, so we only have to show how good the improvement must be, though comment on the value schematic whereby diminishing returns are also far more valuable because of the high surprise (but most of the time this is wasted due to low average entropy). It's why false alarms are not as important, so maybe RES is more important than REL? Bias is welcome? Kernel dressing could be an asymmetric beta function towards the value?\n",
    "\n",
    "Improvements in reliability is more valuable the rarer the event and lower the cost-loss ratio? IG is bits, and each bit is equal, but information can be multiplied by value, hence RES v REL improvements have different value.\n",
    "\n",
    "Inherent uncertainty cancels:\n",
    "\n",
    "$$ \\Delta H_\\times = \\Delta RES \\Delta REL \\\\\n",
    "= I(o{;}f) and what...\n",
    "$$\n",
    "\n",
    "Could normalise the change is Hx by unit resource and value?\n",
    "\n",
    "Would need more ensemble members with finer resolution depending on the forecast lead time of interest? Error growth rates are \"adjoint-like\" at first, but growth exponentially (where REL becomes progressively more important than RES). If we have finer grid, more information is acquired (dt also increases due to CFL), but error growth is faster because of that.\n",
    "\n",
    "Rarer event has more self-entropy and hence needs more information for the event, but less for the average over a period. Show penalties:\n",
    "\n",
    "2x2 matrix of info gain required and the max require, or res v rel value? Info gain per unit resource, as function of configuration, enduser, and flow pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FSS is like reducing the partition - \n",
    "like increasing resolution...KS -> asymptotes. \n",
    "could you estimate where to stop increasing grid res?!\n",
    "Then dp is the accuracy of estimating risk\n",
    "\n",
    "Frigg LSE KSE paper is amazing!\n",
    "\n",
    "Now we have a TRUTH, which is not known. This is discretised\n",
    "with error into chunks identical to the forecast chunks\n",
    "to represent similar errors. Not sure if that is correct but ah \n",
    "\n",
    "Verify for Hx\n",
    "Value of system improvement = Is there diminishing returns situation\n",
    "for improving one or the other, per unit resource (log tail-off wrt. improvements),\n",
    "though skill score is additive (can say skill score because the \n",
    "observation \"climo\" is the same for both model).\n",
    "\n",
    "should be time dimension?\n",
    "dbool = np.diff(results,axis=0)\n",
    "This is -1, 0 (0), 0 (1), +1 showing change between states. There is a\n",
    "correlation in the time dimension but not membership.\n",
    "There is more value in +1 than -1 #\n",
    "there is more value than no-change (1) than no-change (0) (entropy of event diff?)\n",
    "Would you like to increase information entropy now or in future? Have to spend\n",
    "value - do you want more information sooner or less info later (but more valuable)?\n",
    "Want to reduce entropy of rare events v dataset entropy? \n",
    "->Missed hit worse than false alarm\n",
    "However, how to assess this is probabilities? It's the REL value in Hx.\n",
    "\n",
    "Punishment of  \n",
    "\n",
    "REL is the skill of change matrix (punishment in transition for decision tree)\n",
    "RES is the skill of probs per epoch\n",
    "\n",
    "What do we need from this?\n",
    "\n",
    "How does KSE change if the same time period is simulated with\n",
    "a higher dt, or \"windowing\"/chunking? Like error growth being \n",
    "faster at smaller scales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to show how Hx of models changes with the variables. How does KSE relate to \"Bayesian prior\" to reduce ignorance as much as possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work:\n",
    "\n",
    "* How to best extract information at the $\\Delta x$-scale, such as\n",
    "- Machine Learning\n",
    "- Feature identification\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some definitions:\n",
    "\n",
    "| Symbol| Meaning |\n",
    "| ------ | ------- |\n",
    "| $N_e$    | Ensemble membership |\n",
    "| $N_\\alpha$ | Number of \"atoms\" or \"unit time steps\" |\n",
    "| $\\tau$ | Total time (sec) |\n",
    "| $H$ | Average entropy or surprise |\n",
    "| $H_I$ | Self-information or event surprise (bits) |\n",
    "| $H_\\times$ | Cross-entropy (bits) |\n",
    "| $\\textrm{IG}$ | Information gain (bits) |\n",
    "| $\\xi$ | Hazard of interest |\n",
    "| $\\textrm{CC}$ | Channel capacity |\n",
    "| $I(x{;}y)$ | Mutual information between $x$ and $y$ |\n",
    "| $$~D_{KL}(x\\|y)$$ | Kullback-Liebler Divergence between $x$ and $y$ |\n",
    "| $\\Delta x$ | Spatial granulatisation (i.e., horizontal grid-spacing)| \n",
    "| $\\Delta p$ | Probability granularisation (i.e., $N_e^{-1})$|\n",
    "| $\\mathrm{P}(\\xi)$ | (General) probability of event $\\xi$ |\n",
    "| $\\mathrm{P_f}(\\xi)$ | Forecasted probability of event $\\xi$ |\n",
    "| $\\mathrm{P_o}(\\xi)$ | Frequency of observed event $\\xi$ |\n",
    "| $\\mathbb{W}$ | Set of all messages |\n",
    "| $W$ | One original message |\n",
    "| $\\hat{W}$ | One corrupted message |\n",
    "| $\\frac{C}{L}$ | Cost--loss ratio |\n",
    "| $p_{crit}$ | Risk tolerance |\n",
    "| $\\varsigma(W)$ | Surprise reduction operation of message $W$ |\n",
    "| REL | Reliability of the weather model in question (curly m)|\n",
    "| RES | Discrimination (or Brier-type resolution) |\n",
    "\n",
    "Selected mathematical symbols:\n",
    "\n",
    "| Symbol| Meaning |\n",
    "| ------ | ------- |\n",
    "| $\\in$   | Belonging to the set of |\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
